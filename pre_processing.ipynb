{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e512b745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63277015",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./_74429.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d0d21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD = data['news'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4165b910",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\n",
    "    # Standard stopwords\n",
    "    \"a\",\n",
    "    \"about\",\n",
    "    \"above\",\n",
    "    \"after\",\n",
    "    \"again\",\n",
    "    \"against\",\n",
    "    \"all\",\n",
    "    \"am\",\n",
    "    \"an\",\n",
    "    \"and\",\n",
    "    \"any\",\n",
    "    \"are\",\n",
    "    \"arent\",\n",
    "    \"as\",\n",
    "    \"at\",\n",
    "    \"be\",\n",
    "    \"because\",\n",
    "    \"been\",\n",
    "    \"before\",\n",
    "    \"being\",\n",
    "    \"below\",\n",
    "    \"between\",\n",
    "    \"both\",\n",
    "    \"but\",\n",
    "    \"by\",\n",
    "    \"cant\",\n",
    "    \"cannot\",\n",
    "    \"could\",\n",
    "    \"couldnt\",\n",
    "    \"did\",\n",
    "    \"didnt\",\n",
    "    \"do\",\n",
    "    \"does\",\n",
    "    \"doesnt\",\n",
    "    \"doing\",\n",
    "    \"dont\",\n",
    "    \"down\",\n",
    "    \"during\",\n",
    "    \"each\",\n",
    "    \"few\",\n",
    "    \"for\",\n",
    "    \"from\",\n",
    "    \"further\",\n",
    "    \"had\",\n",
    "    \"hadnt\",\n",
    "    \"has\",\n",
    "    \"hasnt\",\n",
    "    \"have\",\n",
    "    \"havent\",\n",
    "    \"having\",\n",
    "    \"he\",\n",
    "    \"hed\",\n",
    "    \"hell\",\n",
    "    \"hes\",\n",
    "    \"her\",\n",
    "    \"here\",\n",
    "    \"heres\",\n",
    "    \"hers\",\n",
    "    \"herself\",\n",
    "    \"him\",\n",
    "    \"himself\",\n",
    "    \"his\",\n",
    "    \"how\",\n",
    "    \"hows\",\n",
    "    \"i\",\n",
    "    \"id\",\n",
    "    \"ill\",\n",
    "    \"im\",\n",
    "    \"ive\",\n",
    "    \"if\",\n",
    "    \"in\",\n",
    "    \"into\",\n",
    "    \"is\",\n",
    "    \"isnt\",\n",
    "    \"it\",\n",
    "    \"its\",\n",
    "    \"itself\",\n",
    "    \"lets\",\n",
    "    \"me\",\n",
    "    \"more\",\n",
    "    \"most\",\n",
    "    \"mustnt\",\n",
    "    \"my\",\n",
    "    \"myself\",\n",
    "    \"no\",\n",
    "    \"nor\",\n",
    "    \"not\",\n",
    "    \"of\",\n",
    "    \"off\",\n",
    "    \"on\",\n",
    "    \"once\",\n",
    "    \"only\",\n",
    "    \"or\",\n",
    "    \"other\",\n",
    "    \"ought\",\n",
    "    \"our\",\n",
    "    \"ours\",\n",
    "    \"ourselves\",\n",
    "    \"out\",\n",
    "    \"over\",\n",
    "    \"own\",\n",
    "    \"same\",\n",
    "    \"she\",\n",
    "    \"shed\",\n",
    "    \"shell\",\n",
    "    \"shes\",\n",
    "    \"should\",\n",
    "    \"shouldnt\",\n",
    "    \"so\",\n",
    "    \"some\",\n",
    "    \"such\",\n",
    "    \"than\",\n",
    "    \"that\",\n",
    "    \"thats\",\n",
    "    \"the\",\n",
    "    \"their\",\n",
    "    \"theirs\",\n",
    "    \"them\",\n",
    "    \"themselves\",\n",
    "    \"then\",\n",
    "    \"there\",\n",
    "    \"theres\",\n",
    "    \"these\",\n",
    "    \"they\",\n",
    "    \"theyd\",\n",
    "    \"theyll\",\n",
    "    \"theyre\",\n",
    "    \"theyve\",\n",
    "    \"this\",\n",
    "    \"those\",\n",
    "    \"through\",\n",
    "    \"to\",\n",
    "    \"too\",\n",
    "    \"under\",\n",
    "    \"until\",\n",
    "    \"up\",\n",
    "    \"very\",\n",
    "    \"was\",\n",
    "    \"wasnt\",\n",
    "    \"we\",\n",
    "    \"wed\",\n",
    "    \"well\",\n",
    "    \"were\",\n",
    "    \"weve\",\n",
    "    \"were\",\n",
    "    \"werent\",\n",
    "    \"what\",\n",
    "    \"whats\",\n",
    "    \"when\",\n",
    "    \"whens\",\n",
    "    \"where\",\n",
    "    \"wheres\",\n",
    "    \"which\",\n",
    "    \"while\",\n",
    "    \"who\",\n",
    "    \"whos\",\n",
    "    \"whom\",\n",
    "    \"why\",\n",
    "    \"whys\",\n",
    "    \"with\",\n",
    "    \"wont\",\n",
    "    \"would\",\n",
    "    \"wouldnt\",\n",
    "    \"you\",\n",
    "    \"youd\",\n",
    "    \"youll\",\n",
    "    \"youre\",\n",
    "    \"youve\",\n",
    "    \"your\",\n",
    "    \"yours\",\n",
    "    \"yourself\",\n",
    "    \"yourselves\",\n",
    "    # Social media filler words\n",
    "    \"rt\",\n",
    "    \"via\",\n",
    "    \"lol\",\n",
    "    \"lmao\",\n",
    "    \"omg\",\n",
    "    \"idk\",\n",
    "    \"tbh\",\n",
    "    \"btw\",\n",
    "    \"pls\",\n",
    "    \"plz\",\n",
    "    \"u\",\n",
    "    \"ur\",\n",
    "    \"r\",\n",
    "    \"imho\",\n",
    "    \"irl\",\n",
    "    \"smh\",\n",
    "    \"fyi\",\n",
    "    \"yea\",\n",
    "    \"yeah\",\n",
    "    \"yup\",\n",
    "    \"nope\",\n",
    "    \"okay\",\n",
    "    \"ok\",\n",
    "    \"k\",\n",
    "    # Noise words\n",
    "    \"breaking\",\n",
    "    \"update\",\n",
    "    \"alert\",\n",
    "    \"exclusive\",\n",
    "    \"viral\",\n",
    "    \"share\",\n",
    "    \"repost\",\n",
    "    \"read\",\n",
    "    \"watch\",\n",
    "    \"click\",\n",
    "    \"follow\",\n",
    "    \"true\",\n",
    "    \"false\",\n",
    "    \"real\",\n",
    "    \"fake\",\n",
    "    \"hoax\",\n",
    "    \"scam\",\n",
    "    # Very frequent low-signal verbs\n",
    "    \"say\",\n",
    "    \"says\",\n",
    "    \"said\",\n",
    "    \"tell\",\n",
    "    \"told\",\n",
    "    \"think\",\n",
    "    \"thought\",\n",
    "    \"know\",\n",
    "    \"known\",\n",
    "    \"report\",\n",
    "    \"reported\",\n",
    "    \"claim\",\n",
    "    \"claimed\",\n",
    "    \"claims\",\n",
    "    \"show\",\n",
    "    \"shown\",\n",
    "    \"shows\",\n",
    "    \"make\",\n",
    "    \"makes\",\n",
    "    \"made\",\n",
    "    \"see\",\n",
    "    \"seen\",\n",
    "    \"look\",\n",
    "    \"looks\",\n",
    "    # Generic nouns that almost never contribute to classification\n",
    "    \"people\",\n",
    "    \"person\",\n",
    "    \"man\",\n",
    "    \"woman\",\n",
    "    \"guy\",\n",
    "    \"guys\",\n",
    "    \"thing\",\n",
    "    \"stuff\",\n",
    "    \"someone\",\n",
    "    \"everyone\",\n",
    "    \"anyone\",\n",
    "    # Misinformation bait\n",
    "    \"wow\",\n",
    "    \"shocking\",\n",
    "    \"unbelievable\",\n",
    "    \"insane\",\n",
    "    \"must\",\n",
    "    \"watch\",\n",
    "    \"truth\",\n",
    "    \"facts\",\n",
    "    \"omfg\",\n",
    "    \"literally\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f41873",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def cleanse(word):\n",
    "  buffer = word.split()\n",
    "  stream = \"\"\n",
    "  for token in buffer:\n",
    "    clean = lem.lemmatize(re.sub(r\"[^a-zA-Z0-9]\", \"\", token).lower()).strip()\n",
    "    if clean not in stopwords:\n",
    "      stream += clean+\" \"\n",
    "  return stream\n",
    "\n",
    "\n",
    "print(cleanse(WORD))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda51de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.DataFrame({\"news\": data['news'].apply(cleanse), \"class\": data['label']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054f29ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.to_csv(\"_74429_V01.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98d1c27",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29bcabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pk\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068c1357",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./_74429_V01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f3c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2751f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['news'] = data['news'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24fb964",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for text in tqdm(data['news'], desc=\"Generating embeddings\"):\n",
    "    emb = model.encode(text)\n",
    "    embeddings.append(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c3bfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('./embeddings.pkl', 'wb')\n",
    "pk.dump({\"embeddings\": embeddings}, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163c16c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = pd.read_csv(\"./_74429_V01.csv\")['class']\n",
    "file = open('./embedding_classes.pkl', 'wb')\n",
    "pk.dump({\"embedding_classes\": classes}, file)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
